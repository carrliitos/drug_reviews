---
title: "Patient Review"
author: "Benzon Carlitos Salazar"
date: "October 22, 2022"
---

```{r setup, warning=FALSE, include=FALSE, echo=FALSE}
library(magrittr)

full <- readRDS(here::here("data", "full.rds"))
```

# Patient reviews

Since there are so many observations, when it comes to tokenization of the 
reviews, my laptop will not be able to handle all of that computing power. So, 
I have decided to only capture the first 1,000 observations in each year and 
work with those.

```{r cut_off}
full <-
  full %>%
  dplyr::group_by(census_year) %>%
  dplyr::slice_head(n = 1000) %>%
  dplyr::ungroup()
```

Let's take a look at 3 sample patients' review text to se what we are dealing 
with.

```{r pat_review_text}
sample_txt <-
  full %>%
  dplyr::select(id, review) %>%
  dplyr::slice_sample(n = 5)
```

**Patient 1's review reads:** `r sample_txt[[2]][1]`

**Patient 2's review reads:** `r sample_txt[[2]][2]`

**Patient 3's review reads:** `r sample_txt[[2]][3]`

**Patient 4's review reads:** `r sample_txt[[2]][4]`

**Patient 5's review reads:** `r sample_txt[[2]][5]`

From this, we get the following initial observations:

* Capitalization of names.
* We get a mix of numeric and character representation of numbers (i.e., 5 Mg vs 
two weeks).
* Word contractions -- it's, didn't, I'm, etc.
* Special characters -- dash (-), ellipsis (...), parentheses.
* Handle HTML character encodings.

## Basic Preprocessing

For some basic string cleaning, we can do the following:

* Remove trailing and leading whitespaces.
* Remove any newlines (`\n`) or tabs (`\t`).
* Normalize capitalizations.
* We convert HTML character encoding to their proper representations using 
`textutils` and enforce consistent character encoding -- `UTF-8`.
* Convert ellipsis

```{r sample_clean_up}
sample_txt_cleanup <- sample_txt[[2]][[3]]

cleaned_txt <-
  sample_txt_cleanup %>%
  textutils::HTMLdecode() %>%
  iconv("UTF-8", sub = "byte") %>%
  tolower() %>%
  stringr::str_replace_all("[^[:alnum:][.]['][!]]", " ") %>%
  stringr::str_trim() %>%
  stringr::str_squish() %>%
  stringr::str_replace_all("[.]{2,3}", ".") # convert ellipsis to a period
```

The following is an example of a cleaned text of a patient's review:

**Original**: `r sample_txt_cleanup`

**Cleaned**: `r cleaned_txt`

```{r full_text_clean}
full$review <- 
  full$review %>%
  textutils::HTMLdecode() %>%
  iconv("UTF-8", sub = "byte") %>%
  tolower() %>%
  stringr::str_replace_all("[^[:alnum:][.]['][!]]", " ") %>%
  stringr::str_trim() %>%
  stringr::str_squish() %>%
  stringr::str_replace_all("[.]{2,3}", ".") # convert ellipsis to a period
```

## Advanced Preprocessing

For the rest of the our preprocessing tasks, we will leverage the `{quanteda}` 
package. `{quanteda}` is an amazing suite for text analytic functions 
[@Quanteda2018].

### Tokenization

Tokenization is the process of converting long strings into a token of words. 
For example, the sentence "Let's go to N.Y.!" can be tokenized to the following 
tokens: {`Let`, `'s`, `go`, `to`, `N.Y.`, `!`}

#### Why do we do it?

Computers don't understand language, but they are really good at counting pieces 
of language. The most informative pieces are (often) words.

### Stemming

Reducing words to their stem.

#### Why do we do it?

When we do not want to dstinguish between different verb forms (walk, walk-ing, 
walk-ed, walk-s) and singular-plural (cat, cat-s).

### Lemmatization

Refers to doing things properly with the use of a vocabulary and morphological 
analysis of words, normally aiming to remove inflectional endings only and to 
return the base or dictionary form of a word, which is known as the *lemma*.

For sentiment analysis, I think it would be more important to lemmatize words 
instead of stem them so that we do not lose the meaning of targeted words.

Lemmatization is more accurate but it requires data about language, and it also 
takes much more time.

```{r lemmatized}
original <- paste0(quanteda::as.tokens(as.list(example$tokens)))

# Stemmed texts
lemmas <- 
  quanteda::tokens_replace(
    quanteda::as.tokens(as.list(example$tokens)), 
    pattern = lexicon::hash_lemmas$token, 
    replacement = lexicon::hash_lemmas$lemma
  ) %>%
  paste0()
```

We can see the original here: `r original`

Then the lemmatized text: `r lemmas`

### Stopwords removal

This is done to remove stopwords.

#### Why do we do it?

Some words are simply not interesting, but do occur often. For instance, words 
such as i, me, my, myself, is, was, ours, ourselves, you, your, etc.

```{r stopwords}
tokens <- 
  quanteda::tokens(
    "I will be a successful data science in the future! Let's go!", 
    remove_punct = TRUE
  )

stopwords_removed <-
  quanteda::tokens_remove(tokens, quanteda::stopwords("english"))
```

Original token: `r tokens`

Stopwords removed: `r stopwords_removed`

## Full processing application

We apply all concepts into our working dataset.

```{r save_and_clean, include=FALSE, echo=FALSE, warning=FALSE}
saveRDS(full, here::here("data", "full_cleaned.rds"))

rm(list = ls())
gc()
```
